#+OPTIONS: ^:nil

* Connector call flow
** ConnectStandalone
*** How to link the work and task
1. ConnectStandard->
     StandaloneHerder.putConnectorConfig->validateConnectorConfig
          AbstractHerder->validateConnectorConfig
                  io.debezium.connector.maria.MySqlConnector -> getConnector
                  Connector connector = plugins().newConnector(connType);
                  Config config = connector.validate(connectorProps);
                  1.1 MySqlConnector -> validate
                  all the props from the config file
                  1.2 MysqlConnector->config()
     StandaloneHerder.putConnectorConfig->startConnector
                 worker.startConnector
                   workerConnector.transitionTo
                     WorkerConnector.start.dostart
                       -> 1.3 MySqlConnector.start
     StandaloneHerder.putConnectorConfig->updateConnectorTasks->recomputeTaskConfigs
                worker.connectorTaskConfigs
                  -> 1.4 MySqlConnector.taskConfigs

2. java.lang.Thread#run()
  WorkerSourceTask
    -> WorkerTask->run->doRun->execute
      -> WorkSourceTask
        -> MySqlConnectorTask.start



createConnectorTasks
Work->startTask
*** How the data push to kafka
BinlogReader
EventBuffer->add->consumeEvent


WorkerTask->run->doRun
  WorkerSourceTask->execute->sendRecords
*** How to push the offset to kafka
  SourceTaskOffsetCommitter->schedule->commit
    WorkerSourceTask->commitOffsets
*** QA
**** =Encountered change event for table bodb100.tablereplistatuswhose schema isn't known to this connector=
***** Root cause
[[./img/schema.01.png]]
;#+INCLUDE: "./org/schema.01.org"
***** Mind Map
#+INCLUDE: "./org/schema.mindMap.org"
  + The schema does not exists in the tableSchema
    - When the schema info should be loaded
      + If new table is added into the config, need to call the schema initialization(ParallelSnapshotReader,newTablesInConfig)
      + Load the schema when the offset from storage is not null
    - Where/How the schema info is loaded from
    - What's the relationship between the schema info and the schema history topic in the kafka
    - ??Schema = table definition + kafka connects

***** Schema load
There are two cases to load the schema info.
  + It's the first time to start the connect, when there is no offset from the storage. The snapshot will be called to initialize it.
  + When the offset from storage is found, it will be initialized from the kafka topic.
  + MySqlTaskContext:new->MySqlSchema:new
    - Get the server name
    - Get the config
    - Get the MySqlJdbcContext
    - Get the topicSelector
    - Initialize the source info
    - Set the excludeGTID and includeGTID
    - =MySqlSchema:new=
    - recordMakers<=schema

[[./img/schema.init.call.flow.png]]

;#+INCLUDE: "../org/schema.init.call.flow.org"

How the MysqlSchema is used in the connector
[[./img/schema.net.png]]
;#+INCLUDE: "./org/schema.net.org"

    #+attr_latex: :environment longtable
    | MySqlTaskContext         | MySqlSchema             | KafkaDatabaseHistory | Comment                 |
    |--------------------------+-------------------------+----------------------+-------------------------|
    | historyExists            | historyExists           | exists               |                         |
    | initializeHistory        | applyDdl                | record               | Please find the comment |
    | initializeHistoryStorage | intializeHistoryStorage | initializeStorage    |                         |
    | loadHistory              | loadHistory             | recover              |                         |
    | shutdown                 | shutdown                | stop                 |                         |

 + initializeHistory
   - Run query against db [SHOW VARIABLES WHERE Variable_name IN ('character_set_server','collation_server')]
   - add [character_set_server/collation_server] to
[[./img/schema.init.seq.png]]
;#+INCLUDE: "./org/schema.init.seq.org"

****** ParallelSnapshotReader:
Todo
****** MySqlConnectorTask
[Only when offset is not null]
start->
  createAndStartTaskContext

***** TODO How the schema history looks like
Now the schema is copied from the file directly, which hide some info.
Prepare one golang to read all the histoic topic to view it clearly.
+ Change character
   #+BEGIN_SRC
{
  "source" : {
    "server" : "bodb100.01"
  },
  "position" : {
    "file" : "BINLOG.001610",
    "pos" : 30884386,
    "gtids" : "1-301-18950598,1303-303-45,18473-18457-680959,1303-303-45,18473-18457-680959",
    "snapshot" : true
  },
  "ddl" : "SET character_set_server=utf8mb4, collation_server=utf8mb4_unicode_ci;"
}
   #+END_SRC
+ Drop table
    #+BEGIN_SRC
{
  "source" : {
    "server" : "bodb100.01"
  },
  "position" : {
    "file" : "BINLOG.001610",
    "pos" : 30884386,
    "gtids" : "1-301-18950598,1303-303-45,18473-18457-680959,1303-303-45,18473-18457-680959",
    "snapshot" : true
  },
  "ddl" : "DROP TABLE IF EXISTS `bodb100`.`51test`"
}
    #+END_SRC
+ Create table
    #+BEGIN_SRC
{
  {
    "server" : "bodb100.01"
  },
  "position" : {
    "file" : "BINLOG.001610",
    "pos" : 31449555,
    "gtids" : "1-301-18951143,1303-303-45,18473-18457-680959,1303-303-45,18473-18457-680959",
    "snapshot" : true
  },
    "databaseName" : "bodb100",
    "ddl" : "CREATE TABLE `trade` (
    `Trade_Pk` bigint(20) NOT NULL,
    `Trade_Date` date NOT NULL,
    ... ...
    $Sell_Trade_Ntl` decimal(20,8) NOT NULL DEFAULT 0.00000000,
    PRIMARY KEY (`Trade_Pk`)
 ,  UNIQUE KEY `Unique_Trade` ($Trade_Time`,`ME_Trade_ID`)
   ) ENGINE=InnoDB DEFAULT $HARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci"
}
#+END_SRC
***** How is the schema inserted into the kafka topic

+ How the schema is inserted into the kafka topic
  So far there are two ways to push the schema into the kafka topic.
  - Copy the schema from db to kafka throgh snapshort reader. One question mark is how determine the GTID to topic when reading the data from db directly
  - Push the change schema into kafka topic from the binlog file.
  - ?? If the schema history is very long, in the initialization phase the load will be very slow since it will have to go through all the schema.
  - If the retention passed, some schema will be roll out from the the schema kafka. How the connect start without such schema info.
[[./img/schema.init.schema.hist.png]]

#+BEGIN_COMMENT
#+BEGIN_SRC plantuml :file ./img/schema.init.schema.hist.png
usecase AbstractReader.start as "AbstractReader
..
start"
usecase SnapshotReader.doStart as "SnapshotReader
..
doStart"

usecase SnapshotReader.execute as "SnapshotReader
..
execute"

usecase BinlogReader.handleQueryEvent as "BinlogReader
..
handleQueryEvent"

usecase MySqlTaskContext.initializeHistory as "MySqlTaskContext
..
initializeHistory"

usecase MySqlTaskContext.loadHistory as "MySqlTaskContext
..
loadHistory"

usecase MySqlConnectorTask.start as "MySqlConnectorTask
..
start"

usecase ReconcilingBinlogReader.setupUnifiedReader as "ReconcilingBinlogReader
..
setupUnifiedReader"

usecase BinlogReader.doStart as "BinlogReader
..
doStart"

usecase BinlogReader.eventHandlers.getOrDefault as "BinlogReader
..
eventHandlers
.getOrDefault"


(AbstractReader.start) -> (BinlogReader.doStart) : Through abstract class
(AbstractReader.start) -> (SnapshotReader.doStart) : Through abstract class
(SnapshotReader.doStart) -> (SnapshotReader.execute)
(BinlogReader.doStart) -> (BinlogReader.handleQueryEvent) : register to eventHandlers
(BinlogReader.doStart) -> (BinlogReader.eventHandlers.getOrDefault)
(SnapshotReader.execute) -> (applyDdl)
(BinlogReader.handleQueryEvent) -> (applyDdl)
(BinlogReader.handleQueryEvent) ..> (eventHandlers)
(eventHandlers) ..> (BinlogReader.eventHandlers.getOrDefault)
(BinlogReader.eventHandlers.getOrDefault) -> (BinlogReader.handleQueryEvent) : Call when eventType is QUERY
(X) -> (MySqlTaskContext.initializeHistory)
(MySqlTaskContext.initializeHistory) -> (applyDdl)
(MySqlTaskContext.loadHistory) -> (applyDdl)
(MySqlConnectorTask.start) -> (MySqlTaskContext.loadHistory)
(ReconcilingBinlogReader.setupUnifiedReader) -> (MySqlTaskContext.loadHistory) : To analyze
(MySqlConnectorTask.start) -> (ReconcilingBinlogReader.setupUnifiedReader) : To analyze

#+END_SRC
#+END_COMMENT


#+BEGIN_COMMENT
#+BEGIN_SRC plantuml :file ./img/schema.init.snap.flow.png
start
:start;
:snapshotReader.doStart;
:snapshotReader.execute;
:;metrics.snapshotStarted;
:disabling autocommit and enabling repeatable read transactions;
:Generate charset-related statement;
:flush and obtain global read lock to prevent writes to database;
:start transaction with consistent snapshot;
:READ BINLOG POSITION;
:Get the list of databases;
:READ TABLE NAMES;
:LOCK TABLES and READ BINLOG POSITION;
partition SchemaSnapshot {
#LightBlue:Transform the current schema so that it reflects the *current* state of the MySQL server's contents.;
  :Set the charset system variables;
  :Add all tables into one list;
  :Generate DROP TABLE ddl statement for each table;
  :Generate DROP DATABASE ddl statement for each database;
  :generate DROP DATABASE IF EXISTS;
  :generate CREATE DATABASE ;
  :generate use;
  repeat
    :run SHOW CREATE TABLE against db;
    :applyddl to schema history;
  repeat while (last table)
  :context.makeRecords.regenerate();
}
:Copy all the data;
:commit or rollback;
:release locks;
#+END_SRC
#+END_COMMENT


#+BEGIN_SRC plantuml :file ./img/schema.snapshot.tree.png
(offset) -> (history) : is null
(history) -> (isSchemaOnlyRecoverySnapshot) : not exists
(isSchemaOnlyRecoverySnapshot) -> (initializeHistoryStorage)  : yes
(initializeHistoryStorage) -> (true)
(isSchemaOnlyRecoverySnapshot) -> (exception) : no

(history) -> (loadHistory) : exists
(loadHistory) -> (isSnapshotInEffect)
(isSnapshotInEffect) -> (isSnapshotNeverAllowed) : yes
(isSnapshotNeverAllowed)  -> (exception) : yes
(isSnapshotNeverAllowed)  -> (true) : no

(isSnapshotInEffect) -> (false) : no

(offset) -> (isSnapshotNeverAllowed-02) : is not null
(isSnapshotNeverAllowed-02) -> (false) : yes
(isSnapshotNeverAllowed-02) -> (true) : no
#+END_SRC


***** How schema only is used to load into the kafka topic.
  + Get the partition + offset of the schema history to start
  + Read the schema from start until the position(partition + offset)

***** How the ConnectRecord push into topic?
producer.send ->
  runtime/WorkerSourceTask.java
  util/KafkaBasedLog.java

How the BlockingConsumer's accept connected to the producer.send
WorkerSourceTask.toSend
WorkerSourceTask.sendRecords

[[./img/record.insert.flow.png]]
;#+INCLUDE: "./org/record.insert.flow.org"

[[; ./img/schema.hist.offset.reader.class.png]]

#+INCLUDE: "./org/schema.hist.offset.reader.class.org"

#+INCLUDE: "./org/record.push.to.kafka.org"

At WorkerSourceTask's toSend
#+BEGIN_SRC
Headers: null
kafkaPartition: null
key
  schema: Schema{bodb100.01.bodb100.tablereplistatus.Key:STRUCT}
  values: [380708]

#+END_SRC

  + bufferSizeForBinlogReader == 0
    - handleEvent
  + bufferSizeForBinlogReader > 0
    - EventBuffer

#+BEGIN_SRC
kafkaOffset: 0
kafkaPartition: 0
key:            Struct{id=202}
keySchema:      Schema{vcdb100.vcdb100.withdrawal.Key:STRUCT}
timestamp:      1570451043033
timestampType:  CreateTime
topic:          vcdb100.vcdb100.withdrawal
value:          Struct{id=202,bank_code=0001,bank_name=\u307f\u305a\u307b\u9280\u884c,bank_name_kana=\uff90\uff7d\uff9e\uff8e,branch_code=100,branch_name=\u672c\u5e97,branch_name_kana=\uff8e\uff9d\uff83\uff9d,bank_account_type=SAVING,bank_account_number=1111111,bank_account_holder=\u30ab\u30af\u3000\u30e9\u30a4,fee=258,total_amount=499742,return_amount=-499742,withdrawal_file_id=7}
valueSchema:    Schema{vcdb100.vcdb100.withdrawal.Value:STRUCT}
#+END_SRC


***** SinkTask
#+INCLUDE: "org/sink.pull.seq.org"

***** snaphot mode
  + Type
    + schema_only
    + schema_only_recovery
    + initial
    + initial_only
    + when_needed
    + never
  + Process
    + Initialization
      - SET TRANSACTION ISOLATION LEVEL REPEATABLE READ
      - non-committing
      - Set character
    + FLUSH TABLES WITH READ LOCK
    + START TRANSACTION WITH CONSISTENT SNAPSHOT
    + readBinlogPosition
      - if schema_only_recovery && no binlogFilename => exception
      - if schema_only_recovery => lastSnapshot: true/nextSnapshot:true
      - else  SHOW MASTER STATUS => (binlogFilename/binlogPosition/Gtidset)
    + SHOW DATABASES
    + LOCK TABLS and READ BINLOG POSITION
      - LOCK TABLES
      - FLUSH TABLES tablename WITH READ LOCK
      - readBinlogPosition
    + Generate schema ddl
      - DROP TABLE IF EXISTS tableName
      - DROP DATABASE IF EXISTS databaseName
      - CREATE DATABASE databaseName
      - USE databaseName
      - SHOW CREATE TABLE to ddl
    + UNLOCK TABLES
    + Copy data to kafka
      - tableId, bufferedRecordQueue  => recordMaker (with schema)
      - USE databaName
      - SHOW TABLE STATUS LIKE tableName
      - SELECT * FROM tableName
    + mark last snapshot
      - lastSnapshot : true / nextSnapshot : false
    + commit/rollback
    + UNLOCK TABLES
    + completeSnapshot
      - lastSnapshot : false / nextSnapshot : false
  + Question
    + How schema_only ignore the data copy
      | SnapshotMode         | Value | Comment               |
      |----------------------+-------+-----------------------|
      | WHEN_NEEDED          | true  | include data snapshot |
      | INITIAL              | true  | no data snapshot      |
      | SCHEMA_ONLY          | false | no data snapshot      |
      | SCHEMA_ONLY_RECOVERY | false | no data snapshot      |
      | NEVER                | false | no data snapshot      |
      | INITIAL_ONLY         | true  | include data snapshot |
    + What's the meaning of lastSnapshot/nextSnaphot
      | lastSnapshot | nextSnapshot | function         | comment                                           |
      |--------------+--------------+------------------+---------------------------------------------------|
      | true         | false        | new              | Initialization                                    |
      | true         | true         | startSnapshot    | a snapshot is being (or has been) started         |
      | true         | false        | markLastSnapshot | a snapshot will be complete after one last record |
      | false        | false        | completeSnapshot | a snapshot has completed                          |
      | nextSnapshot | from source  | setOffset        | ??                                                |
      - lastSnapshot
        The lastSnapshot flag is used to show whethere the data is is pushed to kafka from snapshot.
      - nextSnapshot
        Determine whether a snapshot is currently in effect. The startup will throw exception if meeting the below conditions
        + offsets from storage is not null
        + history topic exists
        + The snapshot is in effect
        + snapshot_mode is never

***** metrics

***** How to convert event to kafka record

 + TableMapEventData
   + tableNumber  : 156424
   + databaseName : vcdb200
   + tableId
     - catalogName: vcdb200
     - id: vcdb200.batch_job_execution
     - schemaName: null
     - tablename: batch_job_execution
   + recordMakers.ssign

SourceRecord{sourcePartition={server=sbtest.01}
, sourceOffset={ts_sec=1570579381, file=BINLOG.001668, pos=16444501, gtids=1-301-19039911,1303-303-45,18473-18457-680960, server_id=301, event=1}}
   ConnectRecord{topic='sbtest.01', kafkaPartition=0
      , key=Struct{databaseName=sbtest}
      , keySchema=Scema{io.debezium.connector.maria.SchemaChangeKey:STRUCT}, value=Struct{source=Struct{version=0.0.1,connector=mysql,name=sbtest.01,server_id=301,ts_sec=1570579381,gtid=1-301-19039912,file=testFileName,pos=16444543,row=0},databaseName=sbtest,ddl=truncate table t1}, valueSchema=Schema{io.debezium.connector.maria.SchemaChangeValue:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}

**** Test
+ SchemaOnly
+ Add table to whitelist
+ multiple database
+ Clean

* UNIT test
    | test case                              | result             | success/all | comment                                      |
    |----------------------------------------+--------------------+-------------+----------------------------------------------|
    | BinlogReaderBufferIT.java              | O                  |             |                                              |
    | ConnectionIT.java                      | O                  |             |                                              |
    | MetadataIT.java                        | O                  |             |                                              |
    | MySqlAntlrDdlParserTest.java           | O                  |             |                                              |
    | MySqlDdlParserTest.java                | O                  |             |                                              |
    | MySqlDateTimeInKeyIT.java              | O                  |             |                                              |
    | MySqlDecimalColumnIT.java              | O                  |             |                                              |
    | MysqlDefaultValueAllZeroTimeIT.java    | O                  |             |                                              |
    | MysqlDefaultValueIT.java               | O                  |             |                                              |
    | MySqlEnumColumnIT.java                 | O                  |             |                                              |
    | MySqlFixedLengthBinaryColumnIT.java    | O                  |             |                                              |
    | MySqlNumericColumnIT.java              | O                  |             |                                              |
    | MySqlTimestampColumnIT.java            | O                  |             |                                              |
    | MySqlUnsignedIntegerIT.java            | O                  |             |                                              |
    | MySqlYearIT.java                       | O                  |             |                                              |
    | MySqlSchemaTest.java                   | O                  | 5/5         |                                              |
    | MySqlSourceTypeInSchemaIT.java         | O                  | 1/1         |                                              |
    | MySqlTableMaintenanceStatementsIT.java | O                  | 1/1         |                                              |
    | MySqlValueConvertersTest.java          | O                  | 4/4         | Removed JSON test                            |
    | TopicNameSanitizationIT.java           | O                  |             |                                              |
    | BinlogReaderIT.java                    | O                  | 10/10       |                                              |
    | MySqlGeometryIT.java                   | O                  | 2/2         |                                              |
    | SourceInfoTest.java                    | O                  | 36/36       | Remove the GTID test                         |
    | ParallelSnapshotReaderTest.java        | O                  | 9/9         |                                              |
    | ChainedReaderTest.java                 | O                  |             |                                              |
    | FiltersTest.java                       | O                  |             |                                              |
    | MySqlConnectorIT.java                  | O                  |             |                                              |
    | MySqlConnectorRegressionIT.java        | O                  |             |                                              |
    | MySqlConnectorTest.java                | O                  |             |                                              |
    | MySqlUnsignedIntegerConverterTest.java | O                  |             |                                              |
    | MySqlGeometryTest.java                 | O                  |             |                                              |
    | ReconcilingBinlogReaderTest.java       | O                  |             |                                              |
    | MysqlDefaultValueTest.java             | O                  | 16/16       | ->AbstractMysqlDefaultValueTest.java         |
    | MysqlAntlrDefaultValueTest.java        | O                  | 19/19       | ->AbstractMysqlDefaultValueTest.java         |
    | ReadBinLogIT.java                      | O                  | 6/6         |                                              |
    | SnapshotReaderIT.java                  | O                  | 8/8         |                                              |
    | MySqlTaskContextTest.java              | O                  | 2/2         | Disable all the other GTID test              |
    | MySqlTaskContextIT.java                | O                  | 2/2         |                                              |
    |----------------------------------------+--------------------+-------------+----------------------------------------------|
    | MySqlConnectorJsonIT.java              | Not test           |             | No need to test json                         |
    | DatabaseDifferences.java               | Not test           |             | Interface                                    |
    | junit/SkipForLegacyParser.java         | Not test           |             | Interface                                    |
    | junit/SkipTestForLegacyParser.java     | Not test           |             | Tool                                         |
    | MySQLConnection.java                   | Not test           |             | Tool                                         |
    | UniqueDatabase.java                    | Not test           |             |                                              |
    | TableConvertersTest.java               | junit=> to convert |             | Disabled                                     |
    | Configurator.java                      | Not test           |             |                                              |
    | AbstractMysqlDefaultValueTest.java     | Not test           |             | Disabled                                     |
    | ZZZGtidSetIT.java                      | Not test           |             | Removed. Mariadb does not this MYSQL feature |

** MyqlDefaultValueIT.java
Comment out the logic to compare the generated value to actual value
#+BEGIN_SRC
//        assertThat(schemaJ.defaultValue()).isEqualTo(
//                MySQLConnection.forTestDatabase(DATABASE.getDatabaseName(), DATABASE.getConnInfo())
//                    .databaseAsserts()
//                    .currentDateTimeDefaultOptional(isoString5)
//        );
#+END_SRC

** BinlogReaderBufferIT.java
  - DDL \\
    The DDL is fetched from test/resource folder
  - Database connection String
    The DB connection info is fetched from system properties.
  - shouldCorrectlyManageRollback
    Problem: All the rollback transaction appeared in the record different from the expectation. \\
    MySql's BEGIN statement  is contained in the QUERY EVENT. In the original source code, the begin transaction is \\
at the QUERY event. Please find the below source code to find it(9.5 final)
    + debezium-connect-mysql->EventBuffer.java->add
    + debezium-connect-mysql->BinlogReader.java->handleQueryEvent
    + mysql-binlog-connector-java->BinaryLogClient->updateGtid
    All these logical have to be moved to the MARIA_GTID_EVENT event as one new transaction. Please find the coressponding \\
source code for the improvement.
  - shouldProcessLargeTransaction
    Problem: Only 3 insert statement appeared in the result different from the expectation of 40
    One parameter called [BUFFER_SIZE_FOR_BINLOG_READER] impact the capacity of the event buffer. This config parameter \\
is used in the debezium-connect-mysql->BinlogReader.java
    #+BEGIN_SRC
      public BinlogReader(String name, MySqlTaskContext context, HaltingPredicate acceptAndContinue, long serverId) {
      ....
      client.registerEventListener(context.bufferSizeForBinlogReader() == 0
          ? this::handleEvent
          : (new EventBuffer(context.bufferSizeForBinlogReader(), this))::add);
    #+END_SRC
    At the same time, in the EventBuffer, it should be switched to switchToBufferFullMode.
** BinlogReaderIT
*** shouldHandleTimestampTimezones
  Have to set the UniqueDatabase's TIMEZONE same to mariadb's time_zone. And one more important difference is that \\
the decimal exceed the definition is truncated rather than round. At the same time, the decimal is only supported 3 \\
digits rather than 6.

  [[https://jira.mariadb.org/browse/MDEV-16991]
*** shouldCreateSnapshotOfSingleDatabase
  Warning message "New transaction started but the previous was not completed, processing the buffer" \\
and "Commit requested but TX was not started before". To resolve the warning message, add the [consumeEvent(event);]
to the else in the mysql-binlog-connector-java->EventBuffer->Add. Mariadb's QUERY event only contain those DDL, commit
and ROLLBACK. Thos queries (insert/delte/update) are in the (WRITE_ROWS/UPDATE_ROWS/DELETE_ROWS).
*** shouldAcceptTls12
**** Set the ssl for mariadb
  - Create the CA certificate
    #+BEGIN_SRC
$cd yomo-connector/yomo-connector-maria/src/test/resources/ssl
$openssl genrsa 2048 > ca-key.pem

$ openssl req -new -x509 -nodes -days 365000 -key ca-key.pem -out ca-cert.pem
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:JP
State or Province Name (full name) []:Tokyo
Locality Name (eg, city) [Default City]:Tokyo
Organization Name (eg, company) [Default Company Ltd]:None
Organizational Unit Name (eg, section) []:Data
Common Name (eg, your name or your server's hostname) []:mdbNode01
Email Address []:mdbNode01@gmail.com
    #+END_SRC
  - Create the server certificate
    #+BEGIN_SRC
$ openssl req -newkey rsa:2048 -days 365000 -nodes -keyout server-key.pem -out server-req.pem
Generating a 2048 bit RSA private key
...............................+++
.....+++
writing new private key to 'server-key.pem'
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:JP
State or Province Name (full name) []:Tokyo
Locality Name (eg, city) [Default City]:Tokyo
Organization Name (eg, company) [Default Company Ltd]:None
Organizational Unit Name (eg, section) []:Data]
Common Name (eg, your name or your server's hostname) []:mdbNode01
Email Address []:mdbNode01@sbibits.com

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:
$ openssl rsa -in server-key.pem -out server-key.pem
writing RSA key
$ls
ca-cert.pem  ca-key.pem  server-key.pem  server-req.pem
$ openssl x509 -req -in server-req.pem -days 365000 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.p
em
Signature ok
subject=/C=JP/ST=Tokyo/L=Tokyo/O=None/OU=Data/CN=mdbNode01/emailAddress=mdbNode01@gmail.com
Getting CA Private Key
$ ls
ca-cert.pem  ca-key.pem  server-cert.pem  server-key.pem  server-req.pem
    #+END_SRC
  - Create the client certificate
    #+BEGIN_SRC
$ openssl req -newkey rsa:2048 -days 365000 -nodes -keyout client-key.pem -out client-req.pem
Generating a 2048 bit RSA private key
........................................................................................+++
.....................................+++
writing new private key to 'client-key.pem'
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:JP
State or Province Name (full name) []:Tokyo
Locality Name (eg, city) [Default City]:Tokyo
Organization Name (eg, company) [Default Company Ltd]:None
Organizational Unit Name (eg, section) []:Data
Common Name (eg, your name or your server's hostname) []:mdbNode01
Email Address []:mdbNode01@gmail.com

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:
$ openssl rsa -in client-key.pem -out client-key.pem
writing RSA key


$ openssl x509 -req -in client-req.pem -days 365000 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out client-cert.p
em
Signature ok
subject=/C=JP/ST=Tokyo/L=Tokyo/O=None/OU=Data/CN=mdbNode01/emailAddress=mdbNode01@gmail.com
Getting CA Private Key
    #+END_SRC
  - Verify the certificates
    #+BEGIN_SRC
$ openssl verify -CAfile ca-cert.pem server-cert.pem client-cert.pem
server-cert.pem: C = JP, ST = Tokyo, L = Tokyo, O = BITS, OU = Data Consulting, CN = mdbNode01, emailAddress = mdbNode01@sbibits.com
error 18 at 0 depth lookup:self signed certificate
OK
client-cert.pem: OK
    #+END_SRC


** MySqlDateTimeInKey
  - The event is as below
    | Seq | EventType     |       GTID | Query           | SNAPSHORT | Addition                 | num |
    |-----+---------------+------------+-----------------+-----------+--------------------------+-----|
    |     | SET           |          - |                 | yes       | SET character_set_server | 1   |
    |     | USE           |          - |                 | yes       | USE database name        | 1   |
    |   1 | Query         | 2-223344-1 | CREATE DATABASE | yes       | DROP DATABASE            | 2   |
    |   2 | Query         | 2-223344-2 | CREATE TABLE    | yes       | DROP TABLE               | 2   |
    |   3 | Annotate_rows | 2-223344-3 | INSERT INTO     |           |                          |     |
    |   4 | Annotate_rows | 2-223344-4 | INSERT INTO     |           |                          |     |

** MySqlDefaultValueIT
*** generatedValueTest
mariadb does not support [generated] with not null option. The value is mandatory.
#+BEGIN_SRC
CREATE TABLE GENERATED_TABLE (
  A SMALLINT UNSIGNED,
  B SMALLINT UNSIGNED AS (2 * A),
  C SMALLINT UNSIGNED AS (3 * A) NOT NULL
);

#+END_SRC
*** columnTypeAndDefaultValueChange
  binlog_rows_query_log_events does not support on the mariadb
** ReadBinLogIT
  - Add wait into recordedEventData
    The CDC is async process. So there is lapse from data inserted into db to be caught in the CDC. In the test, \\
sometimes could not find the rows because of this lapse. What I did is to add wait logical util finding the row \\
match the expectation. Of course, adding the timeout if there is really no data in the CDC.
  - Event sequence
    The event sequence is different from the mysql. Two insert query within one transaction will generate below events.
      | Seq | Event      | Comment |
      |-----+------------+---------|
      |   1 | MARIA_GTID | Begin   |
      |   2 | TABLE_MAP  |         |
      |   3 | WRITE_ROW  |         |
      |   4 | TABLE_MAP  |         |
      |   5 | WRITE_ROW  |         |
      |   6 | XID        | Commit  |
      |-----+------------+---------|
  - Disable shouldFailToConnectToInvalidBinlogFile
    In the maria version, I only use GTID as the replication. The binlog file and position are only used for reference.

** MetadataIT
  The column defined as generated by is lost when the JDBC's readSchema from the DatabaseMetaData. If the CDC really
need this generted from attributed, need to fix it.

** MySqlConnectorIT
Comment out all the assertSourceQuery since the mariadb does not contain ther query in the current event. \\
The MARIA_ANNOTATE_ROWS_EVENT's EventDeserialize needs to be implemented in the maria-binlog-connector-java.
To refer Todo->MARIA_AANOTATE_ROWS_EVENT
*** shouldConsumeAllEventsFromDatabaseUsingSnapshot
Replaced the logic as below, because my understanding of any exception will not push the offset to persisten storage. \\
That's the reason the offset should be same.
#+BEGIN_SRC
assertThat(persistedOffsetSource.binlogPosition()).isGreaterThan(positionBeforeInserts.binlogPosition());
=>
assertThat(persistedOffsetSource.binlogPosition()).isEqualTo(positionBeforeInserts.binlogPosition());
#+END_SRC

** MySqlConnectorRegressionIT
*** shouldConsumeAllEventsFromDatabaseUsingSnapshot
Here is the difference between MYSQL and mariadb that
INSERT IGNORE INTO dbz_114_zerovaluetest VALUES ('0001-00-00', '00:01:00.000', '0001-00-00 00:00:00.000','0001-00-00 00:00:00.000');
  - MYSQL
      |         c1 |          c2 | c3                     | c4                     |
      |------------+-------------+------------------------+------------------------|
      | 0000-00-00 | 00:00:00.00 | 0000-00-00 00:00:00.00 | 0000-00-00 00:00:00.00 |
      | 0000-00-00 | 00:01:00.00 | 0000-00-00 00:00:00.00 | 0000-00-00 00:00:00.00 |
      |------------+-------------+------------------------+------------------------|
  - MARIA
      |         c1 |          c2 | c3                     | c4                     |
      |------------+-------------+------------------------+------------------------|
      | 0000-00-00 | 00:00:00.00 | 0000-00-00 00:00:00.00 | 0000-00-00 00:00:00.00 |
      | 0001-00-00 | 00:01:00.00 | 0000-00-00 00:00:00.00 | 0001-00-00 00:00:00.00 |
      |------------+-------------+------------------------+------------------------|
So in the mariadb, this row is thrown away because of the invalid format.

#+BEGIIN_SRC
assertThat(records.recordsForTopic(DATABASE.topicForTable("dbz_114_zerovaluetest")).size()).isEqualTo(2)
=>
assertThat(records.recordsForTopic(DATABASE.topicForTable("dbz_114_zerovaluetest")).size()).isEqualTo(1)
#+END_SRC



* Summary
** Start cdc with snapshot
  For example, we have 4 create table and 1 alter table schema change, how many events are generated with snapshot?
    - forum
      + all events = snapshot + cdc
      + snapshot = 4 + 2 * # of tables
      + cdc      = 1 + actual schema change
    In the above example, all the snapshot events = 4 + 2 * 4 = 12. And # cdc = 1(db creation) + 4(table creation) + 1(alter table)
So all the events = 12 + 6 = 18.
    - scenario
    | seq | event                    |
    |-----+--------------------------|
    |   1 | set character_set_server |
    |   2 | drop table               |
    |   3 | drop table               |
    |   4 | drop table               |
    |   5 | drop table               |
    |   6 | drop db                  |
    |   7 | create db                |
    |   8 | use db                   |
    |   9 | create table             |
    |  10 | create table             |
    |  11 | create table             |
    |  12 | create table             |
    |-----+--------------------------|
    |  13 | create db                |
    |  14 | create table             |
    |  15 | alter table              |
    |  16 | create table             |
    |  17 | create table             |
    |  18 | create table             |
* Todo
** The nanoseconds
In the current version, the 6 decimals of the nanoseconds are not supported. The nanoseconds are cutoff if the decimal \\
exceed 3 digitas. At the same time, mariadb use cutoff instead of round.
** MARIA_ANNOTATE_ROWS_EVENT
MARIA_ANNOTATE_ROWS_EVENT has not been implemented in the maria-binlog-connector-java. If it need the query, \\
this events should be impletemed in the next version.
** GENERATED
The generated attribute has not been parsed in the current version. To be implemented in the next version.
** Invalid data
How to handle invalid data. like below query
#+BEGIN_SRC
INSERT IGNORE INTO dbz_114_zerovaluetest VALUES ('0001-00-00', '00:01:00.000', '0001-00-00 00:00:00.000','0001-00-00 00:00:00.000');
#+END_SRC
In the SnapshotReader->execute()
#+BEGIN_SRC
                                    // Should catch it and output to some special log file or not? Todo
//                                    catch(Exception e) {
//                                    e.printStackTrace();
//                                    }}
#+END_SRC

** When the offset commit to __consumer in the sink connector
KafkaConsumer->commitAsync->[coordinator.commitOffsetsAsync(new HashMap<>(offsets), callback);]
